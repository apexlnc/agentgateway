#![allow(deprecated)]
#![allow(deprecated_in_future)]

use std::collections::HashMap;
use chrono;

#[allow(deprecated)]
#[allow(deprecated_in_future)]
pub use async_openai::types::ChatCompletionFunctions;
use async_openai::types::Stop;
pub use async_openai::types::{
	ChatChoice, ChatChoiceStream, ChatCompletionAudio, ChatCompletionFunctionCall,
	ChatCompletionMessageToolCall as MessageToolCall, ChatCompletionModalities,
	ChatCompletionNamedToolChoice as NamedToolChoice,
	ChatCompletionRequestAssistantMessage as RequestAssistantMessage,
	ChatCompletionRequestAssistantMessageContent as RequestAssistantMessageContent,
	ChatCompletionRequestDeveloperMessage as RequestDeveloperMessage,
	ChatCompletionRequestDeveloperMessageContent as RequestDeveloperMessageContent,
	ChatCompletionRequestFunctionMessage as RequestFunctionMessage,
	ChatCompletionRequestMessage as RequestMessage,
	ChatCompletionRequestSystemMessage as RequestSystemMessage,
	ChatCompletionRequestSystemMessageContent as RequestSystemMessageContent,
	ChatCompletionRequestToolMessage as RequestToolMessage,
	ChatCompletionRequestToolMessageContent as RequestToolMessageContent,
	ChatCompletionRequestUserMessage as RequestUserMessage,
	ChatCompletionRequestUserMessageContent as RequestUserMessageContent,
	ChatCompletionResponseMessage as ResponseMessage, ChatCompletionStreamOptions as StreamOptions,
	ChatCompletionStreamResponseDelta as StreamResponseDelta, ChatCompletionTool,
	ChatCompletionToolChoiceOption as ToolChoiceOption,
	ChatCompletionToolType as ToolType, CompletionUsage as Usage, CreateChatCompletionRequest,
	CreateChatCompletionResponse as Response, CreateChatCompletionStreamResponse as StreamResponse,
	FinishReason, FunctionCall, FunctionObject, FunctionName, PredictionContent, ReasoningEffort, ResponseFormat, Role,
	ServiceTier, WebSearchOptions,
};
use serde::{Deserialize, Serialize};

#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct Request {
	/// A list of messages comprising the conversation so far. Depending on the [model](https://platform.openai.com/docs/models) you use, different message types (modalities) are supported, like [text](https://platform.openai.com/docs/guides/text-generation), [images](https://platform.openai.com/docs/guides/vision), and [audio](https://platform.openai.com/docs/guides/audio).
	pub messages: Vec<RequestMessage>, // min: 1

	/// ID of the model to use.
	/// See the [model endpoint compatibility](https://platform.openai.com/docs/models#model-endpoint-compatibility) table for details on which models work with the Chat API.
	#[serde(skip_serializing_if = "Option::is_none")]
	pub model: Option<String>,

	/// Whether or not to store the output of this chat completion request
	///
	/// for use in our [model distillation](https://platform.openai.com/docs/guides/distillation) or [evals](https://platform.openai.com/docs/guides/evals) products.
	#[serde(skip_serializing_if = "Option::is_none")]
	pub store: Option<bool>, // nullable: true, default: false

	/// **o1 models only**
	///
	/// Constrains effort on reasoning for
	/// [reasoning models](https://platform.openai.com/docs/guides/reasoning).
	///
	/// Currently supported values are `low`, `medium`, and `high`. Reducing
	///
	/// reasoning effort can result in faster responses and fewer tokens
	/// used on reasoning in a response.
	#[serde(skip_serializing_if = "Option::is_none")]
	pub reasoning_effort: Option<ReasoningEffort>,

	///  Developer-defined tags and values used for filtering completions in the [dashboard](https://platform.openai.com/chat-completions).
	#[serde(skip_serializing_if = "Option::is_none")]
	pub metadata: Option<serde_json::Value>, // nullable: true

	/// Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
	#[serde(skip_serializing_if = "Option::is_none")]
	pub frequency_penalty: Option<f32>, // min: -2.0, max: 2.0, default: 0

	/// Modify the likelihood of specified tokens appearing in the completion.
	///
	/// Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100.
	/// Mathematically, the bias is added to the logits generated by the model prior to sampling.
	/// The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection;
	/// values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
	#[serde(skip_serializing_if = "Option::is_none")]
	pub logit_bias: Option<HashMap<String, serde_json::Value>>, // default: null

	/// Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`.
	#[serde(skip_serializing_if = "Option::is_none")]
	pub logprobs: Option<bool>,

	/// An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.
	#[serde(skip_serializing_if = "Option::is_none")]
	pub top_logprobs: Option<u8>,

	/// The maximum number of [tokens](https://platform.openai.com/tokenizer) that can be generated in the chat completion.
	///
	/// This value can be used to control [costs](https://openai.com/api/pricing/) for text generated via API.
	/// This value is now deprecated in favor of `max_completion_tokens`, and is
	/// not compatible with [o1 series models](https://platform.openai.com/docs/guides/reasoning).
	#[deprecated]
	#[serde(skip_serializing_if = "Option::is_none")]
	pub max_tokens: Option<u32>,

	/// An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and [reasoning tokens](https://platform.openai.com/docs/guides/reasoning).
	#[serde(skip_serializing_if = "Option::is_none")]
	pub max_completion_tokens: Option<u32>,

	/// How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs.
	#[serde(skip_serializing_if = "Option::is_none")]
	pub n: Option<u8>, // min:1, max: 128, default: 1

	#[serde(skip_serializing_if = "Option::is_none")]
	pub modalities: Option<Vec<ChatCompletionModalities>>,

	/// Configuration for a [Predicted Output](https://platform.openai.com/docs/guides/predicted-outputs),which can greatly improve response times when large parts of the model response are known ahead of time. This is most common when you are regenerating a file with only minor changes to most of the content.
	#[serde(skip_serializing_if = "Option::is_none")]
	pub prediction: Option<PredictionContent>,

	/// Parameters for audio output. Required when audio output is requested with `modalities: ["audio"]`. [Learn more](https://platform.openai.com/docs/guides/audio).
	#[serde(skip_serializing_if = "Option::is_none")]
	pub audio: Option<ChatCompletionAudio>,

	/// Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
	#[serde(skip_serializing_if = "Option::is_none")]
	pub presence_penalty: Option<f32>, // min: -2.0, max: 2.0, default 0

	/// An object specifying the format that the model must output. Compatible with [GPT-4o](https://platform.openai.com/docs/models/gpt-4o), [GPT-4o mini](https://platform.openai.com/docs/models/gpt-4o-mini), [GPT-4 Turbo](https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo) and all GPT-3.5 Turbo models newer than `gpt-3.5-turbo-1106`.
	///
	/// Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured Outputs which guarantees the model will match your supplied JSON schema. Learn more in the [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
	///
	/// Setting to `{ "type": "json_object" }` enables JSON mode, which guarantees the message the model generates is valid JSON.
	///
	/// **Important:** when using JSON mode, you **must** also instruct the model to produce JSON yourself via a system or user message. Without this, the model may generate an unending stream of whitespace until the generation reaches the token limit, resulting in a long-running and seemingly "stuck" request. Also note that the message content may be partially cut off if `finish_reason="length"`, which indicates the generation exceeded `max_tokens` or the conversation exceeded the max context length.
	#[serde(skip_serializing_if = "Option::is_none")]
	pub response_format: Option<ResponseFormat>,

	///  This feature is in Beta.
	/// If specified, our system will make a best effort to sample deterministically, such that repeated requests
	/// with the same `seed` and parameters should return the same result.
	/// Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.
	#[serde(skip_serializing_if = "Option::is_none")]
	pub seed: Option<i64>,

	/// Specifies the latency tier to use for processing the request. This parameter is relevant for customers subscribed to the scale tier service:
	/// - If set to 'auto', the system will utilize scale tier credits until they are exhausted.
	/// - If set to 'default', the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.
	/// - When not set, the default behavior is 'auto'.
	///
	/// When this parameter is set, the response body will include the `service_tier` utilized.
	#[serde(skip_serializing_if = "Option::is_none")]
	pub service_tier: Option<ServiceTier>,

	/// Up to 4 sequences where the API will stop generating further tokens.
	#[serde(skip_serializing_if = "Option::is_none")]
	pub stop: Option<Stop>,

	/// If set, partial message deltas will be sent, like in ChatGPT.
	/// Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
	/// as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).
	#[serde(skip_serializing_if = "Option::is_none")]
	pub stream: Option<bool>,

	#[serde(skip_serializing_if = "Option::is_none")]
	pub stream_options: Option<StreamOptions>,

	/// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random,
	/// while lower values like 0.2 will make it more focused and deterministic.
	///
	/// We generally recommend altering this or `top_p` but not both.
	#[serde(skip_serializing_if = "Option::is_none")]
	pub temperature: Option<f32>, // min: 0, max: 2, default: 1,

	/// An alternative to sampling with temperature, called nucleus sampling,
	/// where the model considers the results of the tokens with top_p probability mass.
	/// So 0.1 means only the tokens comprising the top 10% probability mass are considered.
	///
	///  We generally recommend altering this or `temperature` but not both.
	#[serde(skip_serializing_if = "Option::is_none")]
	pub top_p: Option<f32>, // min: 0, max: 1, default: 1

	/// A list of tools the model may call. Currently, only functions are supported as a tool.
	/// Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.
	#[serde(skip_serializing_if = "Option::is_none")]
	pub tools: Option<Vec<ChatCompletionTool>>,

	#[serde(skip_serializing_if = "Option::is_none")]
	pub tool_choice: Option<ChatCompletionToolChoiceOption>,

	/// Whether to enable [parallel function calling](https://platform.openai.com/docs/guides/function-calling/parallel-function-calling) during tool use.
	#[serde(skip_serializing_if = "Option::is_none")]
	pub parallel_tool_calls: Option<bool>,

	/// A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).
	#[serde(skip_serializing_if = "Option::is_none")]
	pub user: Option<String>,

	/// Vendor-specific data for provider compatibility and observability.
	/// Structure: {"provider_name": {"key": "value"}}
	#[serde(skip_serializing_if = "Option::is_none")]
	pub vendor: Option<HashMap<String, serde_json::Value>>,

	/// This tool searches the web for relevant results to use in a response.
	/// Learn more about the [web search tool](https://platform.openai.com/docs/guides/tools-web-search?api-mode=chat).
	#[serde(skip_serializing_if = "Option::is_none")]
	pub web_search_options: Option<WebSearchOptions>,

	/// Deprecated in favor of `tool_choice`.
	///
	/// Controls which (if any) function is called by the model.
	/// `none` means the model will not call a function and instead generates a message.
	/// `auto` means the model can pick between generating a message or calling a function.
	/// Specifying a particular function via `{"name": "my_function"}` forces the model to call that function.
	///
	/// `none` is the default when no functions are present. `auto` is the default if functions are present.
	#[deprecated]
	#[serde(skip_serializing_if = "Option::is_none")]
	pub function_call: Option<ChatCompletionFunctionCall>,

	/// Deprecated in favor of `tools`.
	///
	/// A list of functions the model may generate JSON inputs for.
	#[deprecated]
	#[allow(deprecated)]
	#[allow(deprecated_in_future)]
	#[serde(skip_serializing_if = "Option::is_none")]
	pub functions: Option<Vec<ChatCompletionFunctions>>,
}

impl From<Request> for CreateChatCompletionRequest {
	fn from(req: Request) -> Self {
		#[allow(deprecated)]
		CreateChatCompletionRequest {
			messages: req.messages,
			model: req.model.unwrap_or_default(),
			store: req.store,
			reasoning_effort: req.reasoning_effort,
			metadata: req.metadata,
			frequency_penalty: req.frequency_penalty,
			logit_bias: req.logit_bias,
			logprobs: req.logprobs,
			top_logprobs: req.top_logprobs,
			max_tokens: req.max_tokens,
			max_completion_tokens: req.max_completion_tokens,
			n: req.n,
			modalities: req.modalities,
			prediction: req.prediction,
			audio: req.audio,
			presence_penalty: req.presence_penalty,
			response_format: req.response_format,
			seed: req.seed,
			service_tier: req.service_tier,
			stop: req.stop,
			stream: req.stream,
			stream_options: req.stream_options,
			temperature: req.temperature,
			top_p: req.top_p,
			tools: req.tools,
			tool_choice: req.tool_choice,
			parallel_tool_calls: req.parallel_tool_calls,
			user: req.user,
			web_search_options: req.web_search_options,
			function_call: req.function_call,
			functions: req.functions,
		}
	}
}

#[derive(Debug, Deserialize, Serialize)]
pub struct ChatCompletionErrorResponse {
	pub event_id: Option<String>,
	pub error: ChatCompletionError,
}

#[derive(Debug, Deserialize, Serialize)]
pub struct ChatCompletionError {
	pub r#type: String,
	pub message: String,
	#[serde(skip_serializing_if = "Option::is_none")]
	pub param: Option<String>,
	#[serde(skip_serializing_if = "Option::is_none")]
	pub code: Option<String>,
	#[serde(skip_serializing_if = "Option::is_none")]
	pub event_id: Option<String>,
}

pub const SYSTEM_ROLE: &str = "system";
pub const ASSISTANT_ROLE: &str = "assistant";

pub fn message_role(msg: &RequestMessage) -> &'static str {
	match msg {
		RequestMessage::Developer(_) => "developer",
		RequestMessage::System(_) => "system",
		RequestMessage::Assistant(_) => "assistant",
		RequestMessage::Tool(_) => "tool",
		RequestMessage::Function(_) => "function",
		RequestMessage::User(_) => "user",
	}
}

pub fn message_name(msg: &RequestMessage) -> Option<&str> {
	match msg {
		RequestMessage::Developer(RequestDeveloperMessage { name, .. }) => name.as_deref(),
		RequestMessage::System(RequestSystemMessage { name, .. }) => name.as_deref(),
		RequestMessage::Assistant(RequestAssistantMessage { name, .. }) => name.as_deref(),
		RequestMessage::Tool(RequestToolMessage { tool_call_id, .. }) => Some(tool_call_id.as_str()),
		RequestMessage::Function(RequestFunctionMessage { name, .. }) => Some(name.as_str()),
		RequestMessage::User(RequestUserMessage { name, .. }) => name.as_deref(),
	}
}

pub fn message_text(msg: &RequestMessage) -> Option<&str> {
	// All of these types support Vec<Text>... show we support those?
	// Right now, we don't support
	match msg {
		RequestMessage::Developer(RequestDeveloperMessage {
			content: RequestDeveloperMessageContent::Text(t),
			..
		}) => Some(t.as_str()),
		RequestMessage::System(RequestSystemMessage {
			content: RequestSystemMessageContent::Text(t),
			..
		}) => Some(t.as_str()),
		RequestMessage::Assistant(RequestAssistantMessage {
			content: Some(RequestAssistantMessageContent::Text(t)),
			..
		}) => Some(t.as_str()),
		RequestMessage::Tool(RequestToolMessage {
			content: RequestToolMessageContent::Text(t),
			..
		}) => Some(t.as_str()),
		RequestMessage::User(RequestUserMessage {
			content: RequestUserMessageContent::Text(t),
			..
		}) => Some(t.as_str()),
		_ => None,
	}
}

pub fn max_tokens(req: &Request) -> usize {
	#![allow(deprecated)]
	req.max_completion_tokens.or(req.max_tokens).unwrap_or(4096) as usize
}

pub fn max_tokens_option(req: &Request) -> Option<u64> {
	#![allow(deprecated)]
	req.max_completion_tokens.or(req.max_tokens).map(Into::into)
}

pub fn stop_sequence(req: &Request) -> Vec<String> {
	match &req.stop {
		Some(Stop::String(s)) => vec![s.clone()],
		Some(Stop::StringArray(s)) => s.clone(),
		_ => vec![],
	}
}

/// Convert UniversalMessage back to legacy OpenAI-shaped Response for compatibility
pub fn convert_from_universal_message(msg: UniversalMessage) -> Response {
	let mut content = None;
	let mut tool_calls = Vec::new();
	
	for block in msg.blocks {
		match block {
			ContentBlock::Text { text } => {
				content = Some(text);
			},
			ContentBlock::ToolUse { id, name, input } => {
				tool_calls.push(MessageToolCall {
					id,
					r#type: ToolType::Function,
					function: FunctionCall {
						name,
						arguments: serde_json::to_string(&input).unwrap_or_default(),
					},
				});
			},
			_ => {
				// Skip other content types for now in legacy format
			}
		}
	}
	
	let finish_reason = msg.stop_reason.map(|reason| match reason {
		StopReason::EndTurn => FinishReason::Stop,
		StopReason::MaxTokens => FinishReason::Length,
		StopReason::StopSequence => FinishReason::Stop,
		StopReason::ToolUse => FinishReason::ToolCalls,
		StopReason::ContentFilter => FinishReason::ContentFilter,
		StopReason::PauseTurn => FinishReason::Stop,
	});
	
	let message = ResponseMessage {
		role: Role::Assistant,
		content,
		tool_calls: if tool_calls.is_empty() { None } else { Some(tool_calls) },
		#[allow(deprecated)]
		function_call: None,
		refusal: None,
		audio: None,
	};
	
	let choice = ChatChoice {
		index: 0,
		message,
		finish_reason,
		logprobs: None,
	};
	
	Response {
		id: msg.id,
		object: "chat.completion".to_string(),
		created: chrono::Utc::now().timestamp() as u32,
		model: msg.model,
		choices: vec![choice],
		usage: msg.usage,
		service_tier: None,
		system_fingerprint: None,
	}
}

/// Convert legacy OpenAI-shaped Request to new UniversalRequest for BackendAdapter
pub fn convert_to_universal_request(req: &Request) -> UniversalRequest {
	// Extract system messages
	let mut system = Vec::new();
	let mut messages = Vec::new();
	
	for msg in &req.messages {
		match msg {
			RequestMessage::System(sys_msg) => {
				let text = match &sys_msg.content {
					RequestSystemMessageContent::Text(t) => t.clone(),
					RequestSystemMessageContent::Array(blocks) => {
						// Join text blocks for now
						blocks.iter()
							.filter_map(|block| match block {
								// Assuming there's a text variant - adjust based on actual type
								_ => Some("".to_string()) // TODO: Implement proper text extraction
							})
							.collect::<Vec<_>>()
							.join("\n")
					}
				};
				system.push(ContentBlock::Text { text });
			},
			_ => {
				// Convert other message types to UniversalMessage
				let role = match msg {
					RequestMessage::User(_) => MessageRole::User,
					RequestMessage::Assistant(_) => MessageRole::Assistant,
					RequestMessage::Tool(_) => MessageRole::Tool,
					RequestMessage::System(_) => continue, // Already handled
					RequestMessage::Function(_) => MessageRole::User, // Map deprecated function to user
					RequestMessage::Developer(_) => MessageRole::User, // Map developer to user
				};
				
				// Extract content blocks including tool calls
				let mut blocks = Vec::new();
				if let Some(text) = message_text(msg) {
					blocks.push(ContentBlock::Text { text: text.to_string() });
				}
				
				match msg {
					RequestMessage::Assistant(assistant_msg) => {
						if let Some(tool_calls) = &assistant_msg.tool_calls {
							for tool_call in tool_calls {
								blocks.push(ContentBlock::ToolUse {
									id: tool_call.id.clone(),
									name: tool_call.function.name.clone(),
									input: serde_json::from_str(&tool_call.function.arguments)
										.unwrap_or_else(|_| serde_json::Value::Object(serde_json::Map::new())),
								});
							}
						}
						
						// Handle deprecated function_call field
						#[allow(deprecated)]
						if let Some(function_call) = &assistant_msg.function_call {
							blocks.push(ContentBlock::ToolUse {
								id: format!("func-{}", chrono::Utc::now().timestamp_nanos()),
								name: function_call.name.clone(),
								input: serde_json::from_str(&function_call.arguments)
									.unwrap_or_else(|_| serde_json::Value::Object(serde_json::Map::new())),
							});
						}
					},
					RequestMessage::Tool(tool_msg) => {
						let content_text = match &tool_msg.content {
							RequestToolMessageContent::Text(text) => text.clone(),
							RequestToolMessageContent::Array(parts) => {
								parts.iter()
									.filter_map(|part| match part {
										async_openai::types::ChatCompletionRequestToolMessageContentPart::Text(t) => Some(t.text.as_str()),
									})
									.collect::<Vec<_>>()
									.join("\n")
							}
						};
						
						blocks.push(ContentBlock::ToolResult {
							tool_use_id: tool_msg.tool_call_id.clone(),
							content: vec![ContentBlock::Text { text: content_text }],
							status: Some(ToolResultStatus::Success), // Assume success unless we can parse error info
						});
					},
					RequestMessage::Function(function_msg) => {
						blocks.push(ContentBlock::ToolResult {
							tool_use_id: format!("func-{}", function_msg.name),
							content: vec![ContentBlock::Text { 
								text: function_msg.content.clone().unwrap_or_default() 
							}],
							status: Some(ToolResultStatus::Success),
						});
					},
					_ => {}
				}
				
				messages.push(UniversalMessage {
					id: format!("msg-{}", chrono::Utc::now().timestamp_nanos()),
					model: req.model.clone().unwrap_or_default(),
					role,
					blocks,
					usage: None,
					stop_reason: None,
					vendor: req.vendor.clone(),
				});
			}
		}
	}
	
	// Convert tools
	let tools = req.tools.as_ref().map(|tools| {
		tools.iter().map(|tool| {
			ToolDefinition {
				name: tool.function.name.clone(),
				description: tool.function.description.clone(),
				input_schema: tool.function.parameters.clone().unwrap_or_default(),
			}
		}).collect()
	});
	
	// Build capabilities
	let caps = RequestCapabilities {
		model: req.model.clone().unwrap_or_default(),
		max_tokens: max_tokens_option(req).unwrap_or(4096) as u32,
		temperature: req.temperature,
		top_p: req.top_p,
		top_k: None, // OpenAI doesn't have top_k
		stop_sequences: Some(stop_sequence(req)),
		stream: req.stream,
	};
	
	UniversalRequest {
		system,
		messages,
		tools,
		caps,
		vendor: req.vendor.clone(),
	}
}

// ================================================================
// Universal Architecture Types (Provider-Agnostic)
// ================================================================

/// Universal request format for backend adapters - not coupled to OpenAI format
#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct UniversalRequest {
	/// System messages/prompts
	pub system: Vec<ContentBlock>,
	/// Conversation messages
	pub messages: Vec<UniversalMessage>,
	/// Tool definitions
	pub tools: Option<Vec<ToolDefinition>>,
	/// Model capabilities and configuration
	pub caps: RequestCapabilities,
	/// Vendor-specific data preserved for round-trip fidelity
	pub vendor: Option<HashMap<String, serde_json::Value>>,
}

/// Request capabilities and configuration
#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct RequestCapabilities {
	/// Required: Model to use
	pub model: String,
	/// Required: Maximum tokens to generate
	pub max_tokens: u32,
	/// Optional: Temperature (0.0 to 1.0)
	pub temperature: Option<f32>,
	/// Optional: Top-p (0.0 to 1.0)
	pub top_p: Option<f32>,
	/// Optional: Top-k (minimum 0, Anthropic-specific)
	pub top_k: Option<u32>,
	/// Optional: Stop sequences
	pub stop_sequences: Option<Vec<String>>,
	/// Optional: Enable streaming responses
	pub stream: Option<bool>,
}

/// Tool definition for function calling
#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct ToolDefinition {
	pub name: String,
	pub description: Option<String>,
	pub input_schema: serde_json::Value,
}

/// Universal message format for backend adapters - not coupled to OpenAI format
#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct UniversalMessage {
	pub id: String,
	pub model: String,
	pub role: MessageRole,
	pub blocks: Vec<ContentBlock>,
	pub usage: Option<Usage>,
	pub stop_reason: Option<StopReason>,
	/// Vendor-specific data preserved for round-trip fidelity
	pub vendor: Option<HashMap<String, serde_json::Value>>,
}

/// Universal message roles (provider-agnostic)
#[derive(Clone, Debug, Serialize, Deserialize, PartialEq)]
pub enum MessageRole {
	User,
	Assistant,
	System,
	Tool,
}

/// Universal content blocks for messages
#[derive(Clone, Debug, Serialize, Deserialize)]
pub enum ContentBlock {
	Text { text: String },
	Image { mime: String, data: DataRef },
	ToolUse { id: String, name: String, input: serde_json::Value },
	ToolResult { tool_use_id: String, content: Vec<ContentBlock>, status: Option<ToolResultStatus> },
	Document { name: Option<String>, mime: Option<String>, data: DataRef },
	Thinking { content: String },
}

/// Data reference for media content (images, documents)
#[derive(Clone, Debug, Serialize, Deserialize)]
pub enum DataRef {
	/// URI reference (HTTP/HTTPS URL, S3 URI, etc.)
	Uri(String),
	/// Base64-encoded data
	Base64(String),
}

/// Tool result status
#[derive(Clone, Debug, Serialize, Deserialize, PartialEq)]
pub enum ToolResultStatus {
	Success,
	Error,
}

/// Universal stop reasons (mapped from provider-specific reasons)
#[derive(Clone, Debug, Serialize, Deserialize, PartialEq)]
pub enum StopReason {
	/// Natural end of turn (Anthropic: end_turn)
	EndTurn,
	/// Maximum tokens reached (Anthropic: max_tokens)
	MaxTokens,
	/// Stop sequence encountered (Anthropic: stop_sequence)  
	StopSequence,
	/// Tool use requested (Anthropic: tool_use)
	ToolUse,
	/// Content filtered/guardrail intervened
	ContentFilter,
	/// Future-proofing for pause_turn
	PauseTurn,
}

/// Universal streaming frame types for backend adapters
#[derive(Clone, Debug, Serialize, Deserialize)]
pub enum UFrame {
	MessageStart { 
		id: String, 
		model: String, 
		role: MessageRole 
	},
	BlockStart { 
		idx: usize, 
		kind: BlockKind 
	},
	Delta { 
		idx: usize, 
		text: String 
	},
	ToolUseStart { 
		idx: usize, 
		id: String, 
		name: String 
	},
	ToolUseDelta { 
		idx: usize, 
		json_fragment: Box<serde_json::value::RawValue>
	},
	BlockStop { 
		idx: usize 
	},
	MessageDelta { 
		usage: Usage 
	},
	MessageStop { 
		stop_reason: StopReason 
	},
}

/// Block types for streaming frame starts
#[derive(Clone, Debug, Serialize, Deserialize, PartialEq)]
pub enum BlockKind {
	Text,
	Image,
	ToolUse,
	ToolResult,
	Document,
	Thinking,
}
